---
title: "How functional and reproducible is code shared by ecology papers?"
author: |
  | Kenneth F. Kellner
  | Michigan State University
date: \today
output: pdf_document
header-includes:
    - \usepackage{setspace}\singlespacing
    - \usepackage[margin=1in]{geometry}
---

```{r, echo = FALSE}
# Load libraries
suppressMessages(library(lme4))
suppressMessages(library(DHARMa))
suppressMessages(library(sankey))
suppressMessages(library(knitr))
suppressMessages(library(rmarkdown))
```

```{r, echo=FALSE}
# Read in datasets
# Included papers
incl <- read.csv("included_papers_final.csv")

# Journal information
jour <- read.csv("journal_data.csv")

# Merge journal data into included papers dataset
incl <- merge(incl, jour, by = c("Journal", "Year"))

# Potentially reproducible papers
repr <- read.csv("reproducible_papers_final.csv")
```

# Results

```{r, echo=FALSE}
tabj <- table(incl$Journal)
rng_j <- range(tabj)

tab_y <- table(incl$Year)
rng_y <- range(tab_y)

n_incl <- nrow(incl)
```

We identified `r nrow(incl)+3` papers from `r length(tabj)` journals that met the inclusion criteria.
*Journal of Ecology* was excluded from the final dataset because we found only 2 relevant papers.
Three papers were excluded because they were written by a co-author of this study, resulting a final sample size of `r n_incl`.
The mean number of papers per journal was `r round(mean(tabj))` and the range was `r paste0(rng_j[1], "—", rng_j[2])`.
There were a similar number of papers from each year 2018—2022 (mean `r round(mean(tab_y))`, range `r paste0(rng_y[1], "—", rng_y[2])`).

## Objective 1: Availability of code and data

```{r, echo = FALSE}
possible_reproduce <- incl$All_Data & incl$All_Code
impl_repr <- sum(possible_reproduce)
n_repr <- nrow(repr)
stopifnot(impl_repr == n_repr)
pct_repr <- round(n_repr / n_incl * 100)

pct_appen <- round(mean(repr$Appendix)*100)
pct_dryad <- round(mean(repr$Dryad)*100)
pct_zenodo <- round(mean(repr$Zenodo)*100)

not_incl <- incl[!possible_reproduce,]
mc <- sum(not_incl$Model_Code_Only)
ml <- sum(not_incl$Missing_Broken_Link)
ma <- sum(not_incl$Missing_Appendix)

tab_y <- table(incl$Year, possible_reproduce)
pct_22 <- round(tab_y['2022','TRUE'] / rowSums(tab_y)['2022'] * 100)
```

Of these `r n_incl` papers, `r paste0(n_repr, " (", pct_repr, "%)")` made both data and code available and thus were potentially reproducible (Figure 1).
The most common repositories used were paper appendices (`r paste0(pct_appen, "%")`), Zenodo (`r paste0(pct_zenodo, "%")`), and Dryad (`r paste0(pct_dryad, "%")`).
Common data and code sharing problems (excluding not sharing code or data at all) included supplying only BUGS/Stan model code (`r mc` papers), missing or broken links to outside repositories (`r ml` papers), and missing or incomplete appendices referenced in the text (`r ma` papers).

None of our Objective 1 predictions were supported based on the GLMM.
The probability a paper had both data and code accessible did not differ based on code sharing policy, year, or journal impact factor (Table 1, Figure 1).

## Objective 2: Code functionality

```{r, echo=FALSE}
n_excl <- sum(is.na(repr$All_Code_Runs))
pct_excl <- round(n_excl/n_repr*100)
n_func <- sum(repr$All_Code_Runs, na.rm=TRUE)
n_kept <- n_repr - n_excl
pct_func <- round(n_func / n_kept * 100)

kept_sub <- repr[!is.na(repr$All_Code_Runs),]

n_depr <- sum(kept_sub$Depr_Packages)
n_file <- sum(kept_sub$Miss_File)
n_obj <- sum(kept_sub$Miss_Object)
n_lib <- sum(kept_sub$Miss_Library)

pct_com <- round(mean(kept_sub$Commented, na.rm = TRUE) * 100)

cl_mn <- round(mean(kept_sub$Code_Lines))
cl_sd <- round(sd(kept_sub$Code_Lines))

pkg_mn <- round(mean(kept_sub$Libraries))
pkg_sd <- round(sd(kept_sub$Libraries))

n_script <- sum(kept_sub$Script)
pct_script <- round(n_script / n_kept * 100)

n_mark <- sum(kept_sub$RMarkdown)
pct_mark <- round(n_mark / n_kept * 100)
```

Of the `r n_repr` potentially reproducible papers, we excluded `r n_excl` (`r  pct_excl`%) from further analysis due to issues we blamed on ourselves (extremely long runtimes, RAM limitations, etc.)
On average, papers shared `r cl_mn` ± `r cl_sd` (mean ± standard deviation)  lines of code and depended on `r pkg_mn` ± `r pkg_sd` R libraries.
The majority (`r n_script`, `r pct_script`%) provided code primarily in R script format.
Only a few papers used primarily R Markdown or similar tools (`r n_mark`, `r pct_mark`%).

We were able to successfully execute all the provided code for `r n_func` of `r n_kept` papers (`r pct_func`%; Figure 1).
The most common reasons that code failed to run (there could be multiple reasons) were missing or misnamed files (`r n_file` papers), reliance on deprecated R libraries (`r n_depr` papers), and missing R objects (`r n_obj` papers; Figure 1).

None of our Objective 2 predictions were supported based on the GLMM.
The probability that a paper's code ran successfully was not related to number of code lines, number of libraries used, or code format (Table 2,  Figure 3).
We did not compare commented and non-commented papers as originally planned, because the vast majority of papers (`r pct_com`%) had adequate comments.

## Objective 3: Reproducibility

```{r, echo = FALSE}
n_long <- sum(repr$Runtime_Too_Long & repr$All_Code_Runs, na.rm=TRUE)
pct_long <- round(n_long / n_func * 100)
n_check <- sum(!is.na(repr$Outputs_Match))
n_match <- sum(repr$Outputs_Match, na.rm = TRUE)
pct_match <- round(n_match/n_check*100)
```

Of the `r n_func` papers with functional code, `r n_long` (`r pct_long`%) would have taken too long to fully run due to extensive simulations, MCMC iterations, etc. (Figure 1).
That left `r n_check` papers that we could fully check for reproducibility.
Of these, `r n_match` (`r pct_match`%) reproduced the results reported in the associated paper (Figure 1).
Code from an additional 2 papers produced results similar to the text that probably would have matched if the code had set a random seed.
Given this small sample size, we did not attempt to test our predictions for Objective 3 with this dataset.

# Tables

### Table 1

Estimates from from the generalized linear mixed model (GLMM) of probability a paper provided both code and data, as a function of journal code policy, year, and journal impact factor.
Intercepts were random by journal.

```{r, echo = FALSE}
# Objective 1

# Determine the proportion of papers for which it is possible to attempt to 
# reproduce the results. That is, are both data and code for a given paper available?

# Create Response variable
# Are both data and code for a given paper available?
incl$Data_and_Code <- as.numeric(incl$All_Data & incl$All_Code)

# Predictions
# 1. Papers in journals with data and code availability policies will be more 
# likely to have code and data available
# Covariate: Code_Required (implies data required also)
# 2. Awareness of the importance of reproducibility has been increasing. 
# Thus, year (2018-2022) will have a positive effect on availability of both data and code
# Covariate: Year (standardized)
# 3. Papers in higher-impact journals (as measured by impact factor) 
# will be more likely to have code and data
# Covariate: Impact_Factor

# Fit global GLMM
set.seed(123)
mod1 <- lme4::glmer(Data_and_Code ~ Code_Required + scale(Year) + scale(Impact_Factor) + (1|Journal),
                      family = binomial,      # logistic regression
                      data = incl)

sum1 <- as.data.frame(summary(mod1)$coefficients)
low <- sum1$Estimate - 1.96*sum1$`Std. Error`
up <- sum1$Estimate + 1.96*sum1$`Std. Error`
sum1$`95% CI` <- paste0("(",round(low, 2), " — ", round(up, 2), ")")
sum1$Parameter <- c("Intercept","Code required", "Year", "Impact factor")
sum1 <- sum1[,c("Parameter","Estimate","95% CI")]
sum1$Estimate <- round(sum1$Estimate, 2)
rownames(sum1) <- NULL

rand <- round(sqrt(lme4::VarCorr(mod1)$Journal[1]), 2)
var1 <- data.frame(Parameter = "Journal SD", Estimate = rand, `95% CI` = "",
                   check.names = FALSE)
tab1 <- rbind(sum1, var1)

knitr::kable(tab1)

write.csv(tab1, "Table_1.csv", row.names=FALSE)
```

### Table 2

Estimates from from the generalized linear mixed model (GLMM) of probability a paper's code ran successfully, as a function of number of lines of code, number of R libraries required, and code format (relative to a reference level of R script).
Intercepts were random by journal.

```{r, echo = FALSE}
# For papers with code and data available, are we able to successfully run the code?

# Response variable
#table(repr$All_Code_Runs)

# Predictions
# 1. We expect that the longer and more complex an analysis (as measured crudely 
# in lines of code), the less likely we will be to run the code successfully. 
# The more code you have, the more chances there are to introduce errors.
# Covariate: Code_Lines
# 2. Code that is adequately commented should be more likely to run successfully, 
# since it may indicate that authors worked through the code carefully line by line 
# and confirmed it did what it said it was doing.
# Covariate: Commented
# Almost all papers had a decent number of comments and we realized we
# didn't have a good way of assessing these objectively, so we dropped this covariate
# from the model before analysis
#table(repr$Commented)
# 3. The more outside libraries the code depends on, the less likely the code will be to run.
# This includes all dependencies of libraries called in the script.
# Covariate: Libraries
# 4. Code provided in a format designed for reproducibility, such as Rmarkdown, 
# will be more likely to run successfully than code in a text file (e.g., .R). 
# Code in text files will be more likely to run than code shared in a PDF or Word document.
# Create covariate
code_format <- rep(NA, nrow(repr))
code_format[repr$Word == 1 | repr$PDF == 1] <- "PDF/Word"
code_format[repr$Script == 1] <- "R script"
code_format[repr$RMarkdown == 1 | repr$Other == 1] <- "Rmd/other" # "Other" here is R package
repr$Code_Format <- factor(code_format, levels=c("R script", "Rmd/other", "PDF/Word"))
#table(repr$Code_Format)

# Fit global GLMM
set.seed(123)
mod2 <- lme4::glmer(All_Code_Runs ~ scale(Code_Lines) + scale(Libraries) + Code_Format + (1|Journal),
                      family = binomial,      # logistic regression
                      data = repr)

sum2 <- as.data.frame(summary(mod2)$coefficients)
low <- sum2$Estimate - 1.96*sum2$`Std. Error`
up <- sum2$Estimate + 1.96*sum2$`Std. Error`
sum2$`95% CI` <- paste0("(",round(low, 2), " — ", round(up, 2), ")")
sum2$Parameter <- c("Intercept","Code lines", "Libraries", "Rmd/other", "PDF/Word")
sum2 <- sum2[,c("Parameter","Estimate","95% CI")]
sum2$Estimate <- round(sum2$Estimate, 2)
rownames(sum2) <- NULL

rand2 <- round(sqrt(lme4::VarCorr(mod2)$Journal[1]), 2)
var2 <- data.frame(Parameter = "Journal SD", Estimate = rand2, `95% CI` = "",
                   check.names = FALSE)
tab2 <- rbind(sum2, var2)

knitr::kable(tab2)
write.csv(tab2, "Table_2.csv", row.names=FALSE)
```

\pagebreak


# Figures

### Figure 1

Sankey diagram categorizing code availability and code execution status of papers included in the analysis.
Node labels (and the number of papers at each node) are below the associated node.
Green color indicates a positive outcome (code/data available, code runs) and yellow indicates a negative outcome (no code available, code does not run).

```{r, echo = FALSE, message = FALSE, fig.height=7}

# Set up first node and split
all_papers <- nrow(incl)
all_papers_lab <- paste0("All papers\n(",all_papers,")")

code_and_data <- sum(incl$All_Data & incl$All_Code)
code_and_data_lab <- paste0("\nHas code/\ndata (", code_and_data, ")")
incomplete <- all_papers - code_and_data
inc_lab <- paste0("\nMissing\ncode/data\n(",incomplete,")")

split1 <- data.frame(start_node = rep(all_papers_lab, 2),
                     end_node = c(code_and_data_lab, inc_lab),
                     weight = c(code_and_data, incomplete), 
                     colorstyle="col", col=c("forestgreen", "goldenrod"))

# Set up second node and split
code_runs <- sum(repr$All_Code_Runs, na.rm=TRUE)
code_runs_lab <- paste0("\nCode\nruns (",code_runs,")")
other_issues <- sum(is.na(repr$All_Code_Runs))
other_issues_lab <- paste0("\nOther\nissues (", other_issues, ")")
doesnt_run <- code_and_data - other_issues - code_runs
doesnt_run_lab <- paste0("\nCode\nerrors (",doesnt_run,")") 


split2 <- data.frame(start_node = rep(code_and_data_lab, 3),
                     end_node = c(code_runs_lab, doesnt_run_lab, other_issues_lab),
                     weight = c(code_runs, doesnt_run, other_issues),
                     colorstyle = "col", col=c("forestgreen","goldenrod","gray"))

# Set up third node and splits
data_sub_coderuns <- repr[repr$All_Code_Runs==1 & !is.na(repr$All_Code_Runs),]

too_long <- sum(data_sub_coderuns$Runtime_Too_Long)
too_long_lab <- paste0("Runtime\ntoo long\n(", too_long, ")")
reproduces <- sum(!is.na(data_sub_coderuns$Outputs_Match) & data_sub_coderuns$Outputs_Match==1)
repro_lab <- paste0("Reproduces\n(", reproduces, ")")
doesnt_reproduce <- code_runs - too_long - reproduces
doesnt_repro_lab <- paste0("Doesn't\nreproduce\n(", doesnt_reproduce, ")")

split3 <- data.frame(start_node = rep(code_runs_lab, 3),
                     end_node = c(repro_lab, doesnt_repro_lab, too_long_lab),
                     weight = c(reproduces, doesnt_reproduce, too_long),
                     colorstyle = "col", col=c("forestgreen","goldenrod","gray"))

# Set up fourth node and splits
data_sub_norun <- repr[repr$All_Code_Runs==0 & !is.na(repr$All_Code_Runs),]

dep_pkg <- sum(data_sub_norun$Depr_Packages == 1)
dep_pkg_lab <- paste0("Deprecated\npackage\n(", dep_pkg, ")")
miss_file <- sum(data_sub_norun$Miss_File == 1 & data_sub_norun$Depr_Packages == 0)
miss_file_lab <- paste0("File issue\n(", miss_file, ")")
miss_obj <- sum(data_sub_norun$Miss_Object == 1 & data_sub_norun$Depr_Packages == 0)
miss_obj_lab <- paste0("Missing R\nobject (", miss_obj, ")")
other_error <- doesnt_run - dep_pkg - miss_file - miss_obj
other_error_lab <- paste0("Other error\n(", other_error, ")")

split4 <- data.frame(start_node = rep(doesnt_run_lab, 4),
                     end_node = c(dep_pkg_lab, miss_file_lab,
                                  miss_obj_lab, other_error_lab),
                     weight= c(dep_pkg, miss_file, miss_obj, other_error),
                     colorstyle = "col", col="goldenrod")

# Combine everything
all_splits <- rbind(split1, split2, split3, split4)

# Specify colors of nodes
nodes <- data.frame(id = c(all_papers_lab, code_and_data_lab, 
                           inc_lab, code_runs_lab, doesnt_run_lab,
                           other_issues_lab, repro_lab, doesnt_repro_lab,
                           too_long_lab, dep_pkg_lab, miss_file_lab,
                           miss_obj_lab, other_error_lab),
                    col = c("forestgreen", "forestgreen", "goldenrod", 
                            "forestgreen", "goldenrod", "gray",
                            "forestgreen","goldenrod", "gray",
                            "goldenrod","goldenrod","goldenrod","goldenrod"),
                    cex=1.1, adjy=0.9)

# Make plot input
plot_input <- make_sankey(nodes = nodes, edges = all_splits)

sankey(plot_input, mar = c(0, 3.2, 0, 4.2))

tiff("Figure_1.tiff", height=7, width=7, units='in', res=300, compression='lzw')
sankey(plot_input, mar = c(0, 3.2, 0, 4.2))
nul <- dev.off()
```

###  Figure 2

Effects of journal code policy (A), year (B), and impact factor (C) on proportion of papers that provided complete data and code.
Black points and lines represent estimates and 95% confidence intervals based on the raw data, and gold represents predictions from the GLMM for Objective 1.
In panel C, the raw data points correspond to specific journals identified by acronyms.

```{r, echo = FALSE, fig.height=7}

fig2 <- function(){

par(mar=c(4,2,1,1), oma = c(1,3,0,0))
layout(matrix(c(1, 2,
                3, 3), ncol=2, nrow=2, byrow=TRUE))

# RHS formula
form <- ~Code_Required + scale(Year) + scale(Impact_Factor)

# Code required plot
# Calculate proportion for each code requirement option
tab_req <- table(incl$Code_Required, incl$Data_and_Code)
n <- rowSums(tab_req)
p <- tab_req[,2] / n

# Plot raw data
plot(1:2-0.1, p, ylim = c(0, 0.87), pch=19,
     ylab="Prop. with code and data", cex=1.3, cex.axis=1.2,
     xlab = "Journal code policy", xlim=c(0.5, 2.5), xaxt='n', cex.lab=1.2)
axis(1, at = 1:2, labels = c("Not required", "Required"), cex.axis=1.2)

legend('topright', legend = c("Raw data", "Model prediction"),
       lty = 1, col = c("black", "goldenrod"))

text(0.7, 0.8, "A", cex=2)

mtext("Proportion papers with code and data, and 95% CI", side = 2, line = 1,
      outer = TRUE)

# Plot model predictions

nd <- data.frame(Code_Required = c(0, 1), Year = median(incl$Year), 
                 Impact_Factor = median(incl$Impact_Factor))
mf <- model.frame(form, incl)
mf <- model.frame(terms(mf), nd)
X <- model.matrix(form, mf)
pr1 <- drop(X %*% fixef(mod1))
v <- X %*% vcov(mod1) %*% t(X)
se_fit <- sqrt(diag(v))
est <- plogis(pr1)
low_fit <- plogis(pr1 - 1.96 * se_fit)
up_fit <- plogis(pr1 + 1.96 * se_fit)
points(1:2+0.1, est, cex = 1.3, pch = 19, col='goldenrod')

# Error bars for model prediction
segments(1:2+0.1, low_fit, 1:2+0.1, up_fit, col='goldenrod')
bw <- 0.05
for (i in 1:2){
  segments(i+0.1-bw, low_fit[i], i+0.1+bw, low_fit[i], col='goldenrod')
  segments(i+0.1-bw, up_fit[i], i+0.1+bw, up_fit[i], col='goldenrod')
}

# Error  bars for raw data
# 95% CI
se <- sqrt((p*(1-p))/n)
low <- p - 1.96*se
up <- p + 1.96*se
segments(1:2-0.1, low, 1:2-0.1, up)
bw <- 0.05
for (i in 1:2){
  segments(i-0.1-bw, low[i], i-0.1+bw, low[i])
  segments(i-0.1-bw, up[i], i-0.1+bw, up[i])
}


# Year effect plot
yrs <- 2018:2022

# Calculate proportion in each year
tab_year <- table(incl$Year, incl$Data_and_Code)
n <- rowSums(tab_year)
p <- tab_year[,2] / n

# Plot raw data
plot(yrs, p, ylim = c(0, 0.87), pch=19,
     ylab="Prop. with code and data", cex=1.3, cex.axis=1.2,
     xlab = "Year", xlim=c(2017.75, 2022.25), cex.lab=1.2)

text(2018.25, 0.8, "B", cex=2)

# Plot model predictions
nd <- data.frame(Code_Required = 0, Year = 2018:2022, 
                 Impact_Factor = median(incl$Impact_Factor))
mf <- model.frame(form, incl)
mf <- model.frame(terms(mf), nd)
X <- model.matrix(form, mf)
pr1 <- drop(X %*% fixef(mod1))
v <- X %*% vcov(mod1) %*% t(X)
se_fit <- sqrt(diag(v))
est <- plogis(pr1)
low_fit <- plogis(pr1 - 1.96 * se_fit)
up_fit <- plogis(pr1 + 1.96 * se_fit)
polygon(c(yrs, rev(yrs)), c(low_fit, rev(up_fit)), col='lightgoldenrod1', border=NA)
lines(yrs, est, lty = 2, col='goldenrod')

# Re-plot raw data on top
points(yrs, p, pch = 19, cex = 1.3)

# Error  bars for raw data
# 95% CI
se <- sqrt((p*(1-p))/n)
low <- p - 1.96*se
up <- p + 1.96*se
segments(yrs, low, yrs, up)
bw <- 0.1
for (i in 1:5){
  segments(yrs[i]-bw, low[i], yrs[i]+bw, low[i])
  segments(yrs[i]-bw, up[i], yrs[i]+bw, up[i])
}

# Impact factor effect plot

# Calculate mean impact factor by journal
if_data <- aggregate(jour, by = Impact_Factor~Journal, FUN = mean)
if_data <- if_data[order(if_data$Impact_Factor),]

# Calculate proportion for each journal
tab_journ <- table(incl$Journal, incl$Data_and_Code)
# Sort by impact factor
tab_journ <- tab_journ[if_data$Journal,]
nj <- nrow(tab_journ)
n <- rowSums(tab_journ)
p <- tab_journ[,2] / n

abbrev <- c("JWM","E&E","ECOS","PLOS","SREP","ECOL","BC","CB","MEE")

# Plot raw data
plot(if_data$Impact_Factor, p, ylim = c(0, 0.87), pch=19,
     ylab="Prop. with code and data", cex=1.3, cex.axis=1.2,
     xlab = "Journal impact factor", cex.lab = 1.2,
     xlim = c(min(if_data$Impact_Factor)-0.25, max(if_data$Impact_Factor)+0.25))

text(2.2, 0.8, "C", cex=2)

# Plot model predictions
if_range <- range(if_data$Impact_Factor)
if_seq <- seq(if_range[1], if_range[2], length.out=100)
nd <- data.frame(Code_Required = 0, Year = median(incl$Year), 
                 Impact_Factor = if_seq)
mf <- model.frame(form, incl)
mf <- model.frame(terms(mf), nd)
X <- model.matrix(form, mf)
pr1 <- drop(X %*% fixef(mod1))
v <- X %*% vcov(mod1) %*% t(X)
se_fit <- sqrt(diag(v))
est <- plogis(pr1)
low_fit <- plogis(pr1 - 1.96 * se_fit)
up_fit <- plogis(pr1 + 1.96 * se_fit)
polygon(c(if_seq, rev(if_seq)), c(low_fit, rev(up_fit)), col='lightgoldenrod1', border=NA)
lines(if_seq, est, lty = 2, col='goldenrod')

# Re-plot raw data on top
points(if_data$Impact_Factor, p, pch = 19, cex = 1.3)

# Error  bars for raw data
# 95% CI
xval <- if_data$Impact_Factor
se <- sqrt((p*(1-p))/n)
low <- p - 1.96*se
up <- p + 1.96*se
segments(xval, low, xval, up)
bw <- 0.07
for (i in 1:length(xval)){
  text(xval[i], up[i] + 0.05, abbrev[i])
  segments(xval[i]-bw, low[i], xval[i]+bw, low[i])
  segments(xval[i]-bw, up[i], xval[i]+bw, up[i])
}
}

fig2()

tiff("Figure_2.tiff", height=7, width=7, units='in', res=300,
     compression='lzw')
fig2()
nul <- dev.off()
```

\pagebreak

### Figure 3

Effects of code lines (A), number of required R libraries (B), and code format (C) on proportion of papers for which provided code ran successfully.
Black points and lines represent estimates and 95% confidence intervals based on the raw data, and gold represents predictions from the GLMM for Objective 2.

```{r, echo = FALSE, fig.height = 3.5, fig.width=7}

fig3 <- function(){

par(mar=c(4,2,1,1), oma = c(1,3,0,0), mfrow=c(1,3))

form <- ~scale(Code_Lines) + scale(Libraries) + Code_Format

# Code Lines plot
line_cat <- cut(repr$Code_Lines, c(0, 1000, 2000, 3000, 4000, 50000),
                labels = c("0-1k", "1k-2k", "2k-3k", "3k-4k",
                           "4k+"))

lintab <- table(line_cat, repr$All_Code_Runs)

n <- rowSums(lintab)
p <- lintab[,2] / n
se <- sqrt((p*(1-p))/n)
low <- p - 1.96*se
up <- p + 1.96*se
xval <- levels(line_cat)

xpos <- 1:length(xval)

plot(xpos-0.1, p, ylim = c(0, 0.8), xlim = c(0.75, 5.25), pch=19,
     ylab="Prop. running code", cex=1.3, cex.lab=1.2,
     xlab = "Code lines", xaxt='n')
axis(1, at=xpos, labels=xval)

legend('topright', legend = c("Raw data", "Model prediction"),
       lty = 1, col = c("black", "goldenrod"))

text(1, 0.77, "A", cex=2)

mtext("Prop. running code and 95% CI", side = 2, line = 1,
      outer = TRUE)

# Error bars for raw data
segments(xpos-0.1, low, xpos-0.1, up)
bw <- 0.05
n <- c(paste0("n = ", n[1]), n[2:length(n)])
for (i in 1:5){
  #text(xpos[i]-0.1, up[i] + 0.03, n[i])
  segments(xpos[i]-bw-0.1, low[i], xpos[i]+bw-0.1, low[i])
  segments(xpos[i]-bw-0.1, up[i], xpos[i]+bw-0.1, up[i])
}

# Model prediction at midpoints of bins
nd <- data.frame(Code_Lines = c(500, 1500, 2500, 3500, 4500),
                 Libraries = median(repr$Libraries, na.rm = TRUE),
                 Code_Format = factor("R script", levels=levels(repr$Code_Format)))
mf <- model.frame(form, repr)
mf <- model.frame(terms(mf), nd)
X <- model.matrix(form, mf)
pr2 <- drop(X %*% fixef(mod2))
v <- X %*% vcov(mod2) %*% t(X)
se_fit <- sqrt(diag(v))
est <- plogis(pr2)
low_fit <- plogis(pr2 - 1.96 * se_fit)
up_fit <- plogis(pr2 + 1.96 * se_fit)
points(1:5+0.1, est, cex = 1.3, pch = 19, col='goldenrod')

# Error bars for model prediction
segments(1:5+0.1, low_fit, 1:5+0.1, up_fit, col='goldenrod')
bw <- 0.05
for (i in 1:5){
  segments(i+0.1-bw, low_fit[i], i+0.1+bw, low_fit[i], col='goldenrod')
  segments(i+0.1-bw, up_fit[i], i+0.1+bw, up_fit[i], col='goldenrod')
}

# Libraries
lib_cat <- cut(repr$Libraries, c(0, 50, 100, 150, 1000),
                labels = c("0-50", "51-100", "101-150", "150+"))

libtab <- table(lib_cat, repr$All_Code_Runs)

n <- rowSums(libtab)
p <- libtab[,2] / n
se <- sqrt((p*(1-p))/n)
low <- p - 1.96*se
up <- p + 1.96*se
xval <- levels(lib_cat)

xpos <- 1:length(xval)

# Raw data plot
plot(xpos-0.1, p, ylim = c(0, 0.8), xlim = c(0.75, 4.25), pch=19,
     ylab="Prop. running code", cex=1.3, cex.lab=1.2,
     xlab = "Library dependencies", xaxt='n')
axis(1, at=xpos, labels=xval)

text(1, 0.77, "B", cex=2)

segments(xpos-0.1, low, xpos-0.1, up)
bw <- 0.1
n <- c(paste0("n = ", n[1]), n[2:length(n)])
for (i in 1:5){
  #text(xpos[i], up[i] + 0.03, n[i])
  segments(xpos[i]-bw-0.1, low[i], xpos[i]+bw-0.1, low[i])
  segments(xpos[i]-bw-0.1, up[i], xpos[i]+bw-0.1, up[i])
}

# Model prediction at midpoints of bins
nd <- data.frame(Code_Lines = median(repr$Code_Lines, na.rm = TRUE),
                 Libraries = c(25, 75, 125, 175),
                 Code_Format = factor("R script", levels=levels(repr$Code_Format)))
mf <- model.frame(form, repr)
mf <- model.frame(terms(mf), nd)
X <- model.matrix(form, mf)
pr2 <- drop(X %*% fixef(mod2))
v <- X %*% vcov(mod2) %*% t(X)
se_fit <- sqrt(diag(v))
est <- plogis(pr2)
low_fit <- plogis(pr2 - 1.96 * se_fit)
up_fit <- plogis(pr2 + 1.96 * se_fit)
points(1:4+0.1, est, cex = 1.3, pch = 19, col='goldenrod')

# Error bars for model prediction
segments(1:4+0.1, low_fit, 1:4+0.1, up_fit, col='goldenrod')
bw <- 0.05
for (i in 1:4){
  segments(i+0.1-bw, low_fit[i], i+0.1+bw, low_fit[i], col='goldenrod')
  segments(i+0.1-bw, up_fit[i], i+0.1+bw, up_fit[i], col='goldenrod')
}

# Code format
code_format <- rep(NA, nrow(repr))
code_format[repr$Word == 1 | repr$PDF == 1] <- "PDF/Word"
code_format[repr$Script == 1] <- "R script"
code_format[repr$RMarkdown == 1 | repr$Other == 1] <- "Rmd/other"
code_format <- factor(code_format, levels=c("R script", "Rmd/other",
                                            "PDF/Word"))

formtab <- table(code_format, repr$All_Code_Runs)

n <- rowSums(formtab)
p <- formtab[,2] / n
se <- sqrt((p*(1-p))/n)
low <- p - 1.96*se
up <- p + 1.96*se
xval <- levels(code_format)

xpos <- 1:length(xval)

plot(xpos-0.1, p, ylim = c(0, 0.8), xlim = c(0.75, 3.25), pch=19,
     ylab="Prop. running code", cex=1.3, cex.lab=1.2,
     xlab = "Code format", xaxt='n')
axis(1, at=xpos, labels=xval)

text(1, 0.77, "C", cex=2)

segments(xpos-0.1, low, xpos-0.1, up)
bw <- 0.05
n <- c(paste0("n = ", n[1]), n[2:length(n)])
for (i in 1:3){
  #text(xpos[i], up[i] + 0.03, n[i])
  segments(xpos[i]-bw-0.1, low[i], xpos[i]+bw-0.1, low[i])
  segments(xpos[i]-bw-0.1, up[i], xpos[i]+bw-0.1, up[i])
}

# Model predictions
nd <- data.frame(Code_Lines = median(repr$Code_Lines, na.rm = TRUE),
                 Libraries = median(repr$Libraries, na.rm = TRUE),
                 Code_Format = factor(levels(repr$Code_Format),
                                      levels = levels(repr$Code_Format)))
mf <- model.frame(form, repr)
mf <- model.frame(terms(mf), nd)
X <- model.matrix(form, mf)
pr2 <- drop(X %*% fixef(mod2))
v <- X %*% vcov(mod2) %*% t(X)
se_fit <- sqrt(diag(v))
est <- plogis(pr2)
low_fit <- plogis(pr2 - 1.96 * se_fit)
up_fit <- plogis(pr2 + 1.96 * se_fit)
points(1:3+0.1, est, cex = 1.3, pch = 19, col='goldenrod')

# Error bars for model prediction
segments(1:3+0.1, low_fit, 1:3+0.1, up_fit, col='goldenrod')
bw <- 0.05
for (i in 1:3){
  segments(i+0.1-bw, low_fit[i], i+0.1+bw, low_fit[i], col='goldenrod')
  segments(i+0.1-bw, up_fit[i], i+0.1+bw, up_fit[i], col='goldenrod')
}

}

fig3()

tiff("Figure_3.tiff", height=3.5, width=7, units='in', res=300,
     compression='lzw')
fig3()
nul <- dev.off()
```

\pagebreak

# Supplementary Information

# Figure S1

Results of DHARMa residual tests for the Objective 1 GLMM.

```{r, echo = FALSE}
res1 <- DHARMa::simulateResiduals(mod1)
plot(res1)
```

# Figure S2

Results of DHARMa residual tests for the Objective 2 GLMM.

```{r, echo = FALSE}
res2 <- DHARMa::simulateResiduals(mod2)
plot(res2)
```
